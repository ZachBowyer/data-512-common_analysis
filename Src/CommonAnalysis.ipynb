{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310837c0",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook attempts to answer the question:\n",
    "\"What are the estimated smoke impacts on Leavenworth, Kansas for the last 60 years.\"\n",
    "\n",
    "In this notebook, multiple main actions are done:\n",
    "1. Read wildfire GEOJson data\n",
    "2. Calculate distance from all wildfires to Leavenworth, Kansas by averaging ring distance.\n",
    "3. Save immediate results to new file\n",
    "4. Get the subset of all fires with in 1250 miles of Leavenworth, Kansas\n",
    "5. Create an annual smoke estimate\n",
    "6. Get monthly AQI estimates for Leavenworth Kansas by combining the nearest four station particulate data. \n",
    "7. Create linear regression model to predict the annual smoke estimate, and create ARIMA model to forecast the annual smoke estimate to 2049.\n",
    "8. Create histogram visualization showing the number of fires occuring every 50 miles from Leavenworth up to 1250 miles.\n",
    "9. Create time series showing annual acres burned by wildfires with in 1250 miles of Leavenworth.\n",
    "10. Create time series showing smoke estimate, create time series showing AQI estimate. \n",
    "\n",
    "Warnings:\n",
    "1. Since file sizes are ~3gb, you may run out of memory, especially if you have many duplicates of the same data. This is why we attempt to clear up memory at times.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e693a0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193f45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geojson\n",
    "#!pip install pyproj\n",
    "import os, json, time, sys\n",
    "import geojson\n",
    "import requests\n",
    "import pyproj\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "#from statsmodels.tsa.arima_model import ARIMA\n",
    "#import statsmodels.api as sm\n",
    "from pyproj import Transformer, Geod\n",
    "#from wildfire.Reader import Reader\n",
    "from wildfire.Reader import Reader as WFReader\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ab670",
   "metadata": {},
   "source": [
    "# 1. Read wildfire GEOJson data\n",
    "### Load GEOJSON file reader\n",
    "# -------------------------------------------------------------------------------------------\n",
    "##### DATA:  \n",
    "```\n",
    "Links:   \n",
    "    Homepage:   \n",
    "        https://www.sciencebase.gov/catalog/item/61aa537dd34eb622f699df81    \n",
    "    Full Documentation:  \n",
    "        https://www.sciencebase.gov/catalog/file/get/61aa537dd34eb622f699df81? \n",
    "        f=__disk__d0%2F63%2F53%2Fd063532049be8e1bc83d1d3047b4df1a5cb56f15&transform=1&allowOpen=true \n",
    "Publication date:   \n",
    "    12/08/2021  \n",
    "Description:   \n",
    "    This dataset is a combination of many smaller or incomplete United States fire   \n",
    "    datasets and stores various metadata for historical fires such as: Area, location,   \n",
    "    date, polygon coordinate geometry, etc.   \n",
    "Data format:\n",
    "    {\n",
    "        \"displayFieldName\": ' ',\n",
    "        \"fieldAliases\": { Contains the names of all of the feature attributes seen below, all of type <class 'str'> },\n",
    "        \"geometryType\": String,\n",
    "        \"spatialReference\": {\n",
    "            'wkid': Integer, \n",
    "            'latestWkid': Integer\n",
    "        },\n",
    "        \"fields\": [\n",
    "                      {\n",
    "                          \"name\": <class 'str'>, \n",
    "                           \"type\": <class 'str'>, \n",
    "                           \"alias\": <class 'str'> \n",
    "                      }, \n",
    "                      ...\n",
    "                      {\n",
    "                           \"name\": <class 'str'>, \n",
    "                            \"type\": <class 'str'>, \n",
    "                            \"alias\": <class 'str'> \n",
    "                      }\n",
    "                    ],\n",
    "        \"features\": [\n",
    "                        {\n",
    "                            \"attributes\": {\n",
    "                                \"ObjectID\": <class 'int'>\n",
    "                                \"USGS_Assigned_ID\": <class 'int'>\n",
    "                                'Assigned_Fire_Type: <class 'str'>\n",
    "                                'Fire_Year': <class 'int'>\n",
    "                                'Fire_Polygon_Tier': <class 'int'>\n",
    "                                'Fire_Attribute_Tiers': <class 'str'>\n",
    "                                'GIS_Acres': <class 'float'>\n",
    "                                'GIS_Hectares': <class 'float'>\n",
    "                                'Source_Datasets': <class 'str'>\n",
    "                                'Listed_Fire_Types': <class 'str'>\n",
    "                                'Listed_Fire_Names': <class 'str'>\n",
    "                                'Listed_Fire_Codes': <class 'str'>\n",
    "                                'Listed_Fire_IDs': <class 'str'>\n",
    "                                'Listed_Fire_IRWIN_IDs': <class 'str'>\n",
    "                                'Listed_Fire_Dates': <class 'str'>\n",
    "                                'Listed_Fire_Causes': <class 'str'> \n",
    "                                'Listed_Fire_Cause_Class': <class 'str'>\n",
    "                                'Listed_Rx_Reported_Acres': <class 'NoneType'>\n",
    "                                'Listed_Map_Digitize_Methods': <class 'str'>\n",
    "                                'Listed_Notes': <class 'str'>\n",
    "                                'Processing_Notes': <class 'str'>\n",
    "                                'Wildfire_Notice': <class 'str'>\n",
    "                                'Prescribed_Burn_Notice': <class 'str'>\n",
    "                                'Wildfire_and_Rx_Flag': <class 'NoneType'>\n",
    "                                'Overlap_Within_1_or_2_Flag': <class 'NoneType'>\n",
    "                                'Circleness_Scale': <class 'float'>\n",
    "                                'Circle_Flag': <class 'NoneType'>\n",
    "                                'Exclude_From_Summary_Rasters': <class 'str'>\n",
    "                                'Shape_Length': <class 'float'>\n",
    "                                'Shape_Area': <class 'float'>\n",
    "                            }\n",
    "                            \"geometry\": {\n",
    "                                \"rings\": [[<class 'float'>, <class 'float'>], ..., [<class 'float'>, <class 'float'>]]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "    }\n",
    "Notes:\n",
    "    In this work, the only attributes used from this dataset for analysis are:\n",
    "    ['features']['geometry']['rings'] for coordinate data,\n",
    "    ['features'][\"attributes\"][\"Shape_Area\"] in custom smoke estimate metric\n",
    "    ['features'][\"attributes\"][\"Fire_Year\"] to group fires by year\n",
    "    ['features']['attributes']['Listed_Fire_Names'] - Print out the names of fires for debugging purposes\n",
    "    ['features']['attributes']['GIS_Acres'] - used to visualize number of acres burned for all fires within \n",
    "                                              1250 miles of leavenworth\n",
    "    \n",
    "```\n",
    "# -------------------------------------------------------------------------------------------\n",
    "##### Wildfire reader class:\n",
    "```\n",
    "Location:\n",
    "    /wildfire/Reader.py\n",
    "Description:\n",
    "    A simple streaming reader/loader that is designed to load GeoJSON files. \n",
    "    This class is part of the wildfire user module.\n",
    "    Full documentation is available in the files source code. \n",
    "Copyright:\n",
    "    The WFReader module was created by David W. McDonald.\n",
    "    REVISION: August, 2023\n",
    "    CREATION DATE: August, 2023\n",
    "    Author: David W. McDonald\n",
    "    A simple streaming reader/loader that is designed to load GeoJSON files. \n",
    "    This class is part of the wildfire user module.\n",
    "    Copyright by Author. All rights reserved. Not for reuse without express permissions.\n",
    "```\n",
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9708f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in geojson file into <class 'dict'>, which has a total of 135061 fires\n"
     ]
    }
   ],
   "source": [
    "# Load data into dictionary\n",
    "data_filepath = \"./../Data/USGS_Wildland_Fire_Combined_Dataset.json\"\n",
    "geojson_file = open(data_filepath,\"r\")\n",
    "gj_data = geojson.load(geojson_file)\n",
    "geojson_file.close()\n",
    "print(f\"Loaded in geojson file into {type(gj_data)}, which has a total of {len(gj_data['features'])} fires\")\n",
    "\n",
    "# Create wildfire reader class\n",
    "wfreader = WFReader(data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8644415",
   "metadata": {},
   "source": [
    "### Get fire data into list of dictionaries\n",
    "In this code block we will use a <class 'wildfire.Reader.Reader'> to remove a lot of unnecessary fields  \n",
    "For example, we are only keeping   \n",
    "['features'][\"attributes\"]  \n",
    "['features'][\"geometry\"]  \n",
    "Besides that, the schema for each entry follows the schema described above for the attributes that remain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44485a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 135061/135061 [09:22<00:00, 240.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a total of 135061 wildfires\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We know this is how many features are in the dataset, this should be changed for other datasets\n",
    "MAX_FEATURE_LOAD = 135061\n",
    "wildfires = []\n",
    "feature_count = 0\n",
    "\n",
    "# A rewind() on the reader object makes sure we're at the start of the feature list\n",
    "wfreader.rewind()\n",
    "\n",
    "# Now, read through each of the features, saving them as dictionaries into a list\n",
    "feature = wfreader.next()\n",
    "for i in tqdm(range(135061), position=0, leave=True):\n",
    "    wildfires.append(feature)\n",
    "    feature_count += 1\n",
    "    feature = wfreader.next()\n",
    "print(f\"Loaded a total of {feature_count} wildfires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8901a0",
   "metadata": {},
   "source": [
    "# 2. Calculate distance from all wildfires to Leavenworth, Kansas by averaging ring distance.\n",
    "### For each fire calculate average ring distance from Leavenworth Kansas\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "Distance calculation\n",
    "```\n",
    "In this work, a design decision was made to refer to the 'distance' between a wildfire and a city as \n",
    "the average distance between the coordinate of the city and all polygon coordinates of the wildfire.\n",
    "\n",
    "This decision was made because average distance is easy to calculate and simple to understand. \n",
    "```\n",
    "# -------------------------------------------------------------------------------------------\n",
    "Coordinate conversions\n",
    "```\n",
    "The data received from https://www.sciencebase.gov/catalog/item/61aa537dd34eb622f699df81 is naturally in the ESRI:102008 format.\n",
    "For this work we want to convert it to EPSG:4326 coordinates because of two main reasons:\n",
    "1. EPSG:4326 coordinates are much more human-readable than ESRI:102008 coordinates\n",
    "2. Previous modules worked on by Dr. David McDonald assumed coordinates to be in EPSG:4326, so to have formats play nice\n",
    "   this work decided to stick with EPSG:4326.\n",
    "\n",
    "Example coordinate conversions:\n",
    "[-1883775.5960000008, 1194154.1922999993] -> [47.82096504186229, -123.03391060341669]\n",
    "[-1883782.4154000003, 1194101.5089999996] -> [47.82051986874537, -123.033793425185]\n",
    "```\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "Coordinate systems:\n",
    "```\n",
    "\"\n",
    "ESRI:102008:\n",
    "\n",
    "Name: North America Albers Equal Area Conic\n",
    "Authority: ESRI (Environmental Systems Research Institute)\n",
    "Projection Type: Conic\n",
    "Units: Meter\n",
    "The ESRI:102008 coordinate system is specifically designed for North America and is based on the Albers Equal Area Conic projection. This projection is conformal, meaning that local angles are preserved, and it's equal area, meaning that the relative size of features on the map is accurate. This makes it suitable for mapping areas with significant north-south and east-west extents, such as the United States and Canada.\n",
    "\n",
    "EPSG:4326:\n",
    "\n",
    "Name: WGS 84\n",
    "Authority: EPSG (European Petroleum Survey Group)\n",
    "Projection Type: Geographic (unprojected)\n",
    "Units: Decimal degrees\n",
    "The EPSG:4326 coordinate system is based on the World Geodetic System of 1984 (WGS 84). It uses a simple latitude and longitude grid to represent locations on the Earth's surface. This coordinate system is widely used as a standard for representing global data, and it is commonly used in web mapping applications. The units are in decimal degrees, where positive values indicate north and east, and negative values indicate south and west.\n",
    "\n",
    "In summary, ESRI:102008 is a projected coordinate system suitable for mapping North America with units in meters, while EPSG:4326 is an unprojected coordinate system using a latitude and longitude grid with units in decimal degrees, commonly used for global mapping and navigation. The choice between these coordinate systems depends on the specific needs of a given GIS project and the region of interest.\n",
    "\"\n",
    "```\n",
    "Disclaimer\n",
    "```\n",
    "The above coordinate summaries were generated by ChatGPT version 3.5, which can be found at https://chat.openai.com/.\n",
    "The specific prompt used was:\n",
    "\"Provide an explanation of both ESRI:102008 and EPSG:4326 coordinate systems\"\n",
    "Information may be partially or completely incorrect\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e39c71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 135026/135026 [4:00:07<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  0  removed entries that could not have distance calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_ring_to_epsg4326(coordinates=None):\n",
    "    # Description:\n",
    "    #    Converts a list of ESRI:102008 coordinates to EPSG:4326 coordinates\n",
    "    # Inputs:\n",
    "    #    coordinates - List of ESRI:102008 coordinates that will be transformed to EPSG:4326 (List of lists)\n",
    "    # Output:\n",
    "    #    List of coordinates in EPSG:4326 (List of lists)\n",
    "    \n",
    "    # Store list of converted coordinates\n",
    "    converted_coords = list()\n",
    "\n",
    "    # Use pyproj transformer that converts from ESRI:102008 to EPSG:4326 to transform the list of coordinates\n",
    "    to_epsg4326 = Transformer.from_crs(\"ESRI:102008\",\"EPSG:4326\")\n",
    "\n",
    "    # Transform each ESRI:102008 (x,y) coord into a decimal degree (lat,lon), then put it in a list\n",
    "    for coord in coordinates:\n",
    "        lat,lon = to_epsg4326.transform(coord[0],coord[1])\n",
    "        converted_coords.append([lat, lon])\n",
    "    return converted_coords\n",
    "\n",
    "def average_distance_from_place_to_fire_perimeter(target_coordinates=None, wildfire_coordinates=None):\n",
    "    # Description:\n",
    "    #    Calculates the average miles from a list of coordinates to target coordinates\n",
    "    # Inputs:\n",
    "    #    target_coordinates - list or tuple with 2 items (lat,lon) in decimal degrees EPSG:4326\n",
    "    #    wildfire_coordinates - List of decimal degree coordinates (List of lists) EPSG:4326\n",
    "    # Output:\n",
    "    #    Average miles from wildfire_coordinates coordinates to the target coordinates\n",
    "    # Notes:\n",
    "    #    Modification of function created by David W. McDonald.\n",
    "    #    This code was developed by David W. McDonald for use in Data 512,\n",
    "    #    a course in the UW MS Data Science degree program. This code is \n",
    "    #    provided under the Createive Commons CC-BY license. Revision 1.0 - \n",
    "    #    August 13, 2023.\n",
    "    \n",
    "    # Convert wildfire polygon coordinates from ESRI:102008 to EPSG:4326\n",
    "    EPSG_coordinates = convert_ring_to_epsg4326(wildfire_coordinates)\n",
    "    \n",
    "    # Create a epsg4326 compliant object - which is what the WGS84 ellipsoid is. Used for distance comparisons\n",
    "    geodcalc = Geod(ellps='WGS84')\n",
    "    \n",
    "    # Get list of meter distances between each point coordinates and the target coordinates\n",
    "    distances_in_meters = list()\n",
    "    for point in EPSG_coordinates:\n",
    "        d = geodcalc.inv(target_coordinates[1], target_coordinates[0], point[1], point[0]) #Distance calculation\n",
    "        distances_in_meters.append(d[2])\n",
    "        \n",
    "    # Convert list of meter distances to list of mile distances\n",
    "    distances_in_miles = [meters*0.00062137 for meters in distances_in_meters]\n",
    "    \n",
    "    # The esri polygon shape (the ring) requires that the first and last coordinates \n",
    "    # be identical to 'close the region. We remove one of them so that we don't bias \n",
    "    # our average by having two of the same point.\n",
    "    distances_in_miles_no_dup = distances_in_miles[1:]\n",
    "    \n",
    "    # Get the average distance in miles\n",
    "    average = sum(distances_in_miles_no_dup)/len(distances_in_miles_no_dup)\n",
    "    return average\n",
    "\n",
    "def calculate_fire_distances(wildfires, target_coordinates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "       Given list of dictionaries of wildfire data, add a new field \n",
    "       called 'distance_from_Leavenworth\", to each dictionary that is \n",
    "       the average distance between the coordinates of Leavenworth, Kansas\n",
    "       and the polygon coordinate points of the wildfire. If a wildfire doesn't \n",
    "       have any coordinates associated with it, we will remove it from the argument\n",
    "       dictionary.\n",
    "    Inputs:\n",
    "        wildfires - List of dictionaries (Format describes in previous documentation)\n",
    "        target_coordinates - Tuple of lat,lon WSG84 coordinates (float)\n",
    "    Output:\n",
    "        List of dictionary items that were removed because a distance could not be calculated\n",
    "    \"\"\"\n",
    "    #Store all wildfires that could not be processed in a list of dictionaries\n",
    "    log_deleted_entries = []\n",
    "    \n",
    "    # Make a progress bar that is linked to how many fires have been looked at\n",
    "    for i in tqdm(range(len(wildfires)), position=0, leave=True):\n",
    "        \n",
    "        # Get wildfire info\n",
    "        wildfire = wildfires[i]\n",
    "        \n",
    "        #Log entries where distance cannot be calculated due to missing coordinate information. \n",
    "        if('rings' not in wildfire['geometry'].keys()):\n",
    "            print(\"missing info\")\n",
    "            log_deleted_entries.append([i, wildfire])\n",
    "            continue\n",
    "        \n",
    "        # If coordinate information is present,\n",
    "        # Calculate distance between coordinates and city\n",
    "        ring_data = wildfire['geometry']['rings'][0]\n",
    "        distance = average_distance_from_place_to_fire_perimeter(target_coordinates, ring_data)\n",
    "        wildfire['distance_from_Leavenworth'] = distance\n",
    "    \n",
    "    #Return all deleted dictionaries\n",
    "    return log_deleted_entries\n",
    "\n",
    "# EPSG/WSG84 coordinates of the city: Leavenworth, Kansas (LAT, LON)\n",
    "CITY_COORDS = (39.313015, -94.941147)\n",
    "\n",
    "# Document out how many fires had to skipped over\n",
    "deleted_entries = calculate_fire_distances(wildfires, CITY_COORDS)\n",
    "print(\"There were \", len(deleted_entries), \" removed entries that could not have distance calculated.\")\n",
    "\n",
    "# Remove all entries that have no coordinate information:\n",
    "for log in deleted_entries:\n",
    "    index = log[0]\n",
    "    del wildfires[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bf678",
   "metadata": {},
   "source": [
    "# 3. Save immediate results to new file\n",
    "The reason we are doing this is because it takes 2-3 hours at a minimum to re-calculate distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31944e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save these distances to a new file so we don't have to redo the calculations at a later time\n",
    "with open('./../Data/USGS_Wildland_Fire_Combined_WithDistances.json', 'w') as f:\n",
    "    json.dump({\"data\": wildfires}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452e89a",
   "metadata": {},
   "source": [
    "# 4. Get the subset of all fires with in 1250 miles of Leavenworth, Kansas\n",
    "\n",
    "Why 1250 miles? - This was an arbitrary cutoff point specified by course staff for our subsequent analyses.\n",
    "\n",
    "### Load in data from new file \n",
    "This is so you can rerun the notebook and skip the step that takes a really long time.  \n",
    "Additionally, clear the old data if it is still there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file, and clear out old information to save on space\n",
    "#del wildfires\n",
    "#del gj_data\n",
    "with open('./../Data/USGS_Wildland_Fire_Combined_WithDistances.json') as f:\n",
    "    wildfire_data = json.load(f)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fires_in_distance(wildfires, miles):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Given list of dictionaries of wildfire data, return all fires that meet\n",
    "        the condition that their distance_from_Leavenworth attribute is less than\n",
    "        the input argument 'miles'.\n",
    "    Input:\n",
    "        wildfires - List of dictionaries, documentation previously supplied\n",
    "        miles - Float/Integer\n",
    "    Output:\n",
    "        List of dictionaries (Subset of input argument)\n",
    "    \"\"\"\n",
    "    # Stores all fire data that falls within the specified mile range (0 - miles)\n",
    "    fires_in_distance = []\n",
    "    \n",
    "    # Check distance for each wildfire\n",
    "    for i in range(len(wildfires)):\n",
    "        wildfire = wildfires[i]\n",
    "        wildfire_name = wildfire['attributes']['Listed_Fire_Names'].split(',')[0]\n",
    "        try:\n",
    "            ring_data = wildfire['geometry']['rings'][0] #This doesnt exist for all fires\n",
    "            distance = wildfire['distance_from_Leavenworth']\n",
    "            if(distance <= 1250):\n",
    "                fires_in_distance.append(wildfire)\n",
    "        except:\n",
    "            print(\"No data for\", wildfire['attributes']['Listed_Fire_Names'].split(',')[0])\n",
    "    return fires_in_distance\n",
    "\n",
    "city_fires = get_fires_in_distance(wildfire_data, 1250)\n",
    "print(\"There are a total of\", len(city_fires), \"within 1250 miles of leavenworth, kansas\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427cdfc",
   "metadata": {},
   "source": [
    "### Show example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339c5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(wildfire_data[0],indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac30bc4",
   "metadata": {},
   "source": [
    "### Put fires into dictionary grouped by \"Fire_Year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_grouped_by_year = {}\n",
    "for fire in wildfire_data:\n",
    "    year = str(fire[\"attributes\"][\"Fire_Year\"])\n",
    "    if(year not in fires_grouped_by_year.keys()):\n",
    "        fires_grouped_by_year[year] = []\n",
    "        fires_grouped_by_year[year].append(fire)\n",
    "    else:\n",
    "        fires_grouped_by_year[year].append(fire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8490e1",
   "metadata": {},
   "source": [
    "# 5. Create an annual smoke estimate\n",
    "The estimate for annual smoke that I came up with is very simple. \n",
    "In this estimate I try to incorporate the volume of smoke burned by wood, the amount of \n",
    "wood burned, and the distance from the wildfire to Leavenworth. \n",
    "Unfortunately, this estimate is probably very inaccurate.\n",
    "\n",
    "Formula: Smoke_impact_from_fire = (area_forest_burned * 10 * 87.5) / (distance_from_city^5)\n",
    "To get the annual smoke estimate, we sum the smoke_impact_from_fire over every fire.\n",
    "\n",
    "Assumptions made:\n",
    "1. The volume of smoke produced by wood is 87.5 M^2 per kg. \n",
    "http://virtual.vtt.fi/virtual/innofirewood/stateoftheart/database/burning/burning.html#:~:text=In%20well%20ventilated%20conditions%2C%20the,the%20smoke%20production%20of%20wood.  \n",
    "2. There is 10 kilograms of wood on average per square meter in a forest. (No source)\n",
    "3. Smoke dispersion is inversely proportional to the distance to the fifth power. (Did this to keep relative scale low)\n",
    "\n",
    "Reasons this estimate is bad:\n",
    "1. The volume of smoke produced by wood depends on the type of tree.\n",
    "2. More than just wood burns in a wildfire. \n",
    "3. The amount of wood on average per square meter in a forest varies on the type of flora present, the season, and other various factors.\n",
    "4. Smoke dispersion is much more complicated than what I have proposed. There have been many studies on atmospheric transport models, which are much better estimates than what we have. However data for atmospheric transport models probably does not exist for our data (especially for the old stuff). \n",
    "5. Other factors that I'm not aware of\n",
    "\n",
    "###### Notes:  \n",
    "Overall, I decided the smoke estimate should be summed during the year because data is sparse. This means if we were to use a finer time scale we would end up with a lot of missing data that we would either have to set as an average or 0 in order to do time-series analysis. \n",
    "\n",
    "I did not see a reason to only use wildfires from the fire season, as smoke can effect a city at any time of the year. \n",
    "\n",
    "I looked for sources talking about the amount of wood per square meter in a forest but could not find anything. Therefore I arbitrarily picked a number, as the amount of smoke would scale linearly, so in terms of tracking relative change it wouldn't affect much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ab41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_smoke_estimates = {}\n",
    "start_year = 1963\n",
    "end_year = 2023\n",
    "\n",
    "for year in range(1963, 2021):\n",
    "    total_smoke_volume_estimate = 0\n",
    "    if(str(year) not in fires_grouped_by_year.keys()):\n",
    "        annual_smoke_estimates[str(year)] = {\"total_smoke_volume_estimate\": 0}\n",
    "        continue\n",
    "    for fire in fires_grouped_by_year[str(year)]:\n",
    "        try:\n",
    "            area_of_forest_burned = fire[\"attributes\"][\"Shape_Area\"] #Square meters\n",
    "            distance_from_leavenworth = fire[\"distance_from_Leavenworth\"]\n",
    "            if(distance_from_leavenworth > 1250):\n",
    "                continue\n",
    "        \n",
    "            #Making bad assumption that there is 10kg of wood on average per square meter in a forest\n",
    "            wood_burned_estimate = area_of_forest_burned * 10 #In Kilograms\n",
    "            \n",
    "            #Assumption from source above to calculate smoke volume from burning wood\n",
    "            smoke_volume_estimate = wood_burned_estimate * 87.5 #Meters squared\n",
    "        \n",
    "            #Making bad assumption that smoke dispersion is inversely proportion to the exponential of distance\n",
    "            smoke_volume_estimate = smoke_volume_estimate / (int(distance_from_leavenworth)**5)\n",
    "            total_smoke_volume_estimate += smoke_volume_estimate\n",
    "            annual_smoke_estimates[str(year)] = {\n",
    "                \"total_smoke_volume_estimate\": total_smoke_volume_estimate\n",
    "            }\n",
    "        except:\n",
    "            print(\"Skipped over\", fire['attributes']['Listed_Fire_Names'].split(',')[0])\n",
    "\n",
    "print(annual_smoke_estimates)\n",
    "# Summation\n",
    "# Volume of wood burned * volume of smoke produced by burning X volume of wood\n",
    "# / distance from city\n",
    "# Divide by N\n",
    "# Divide by max\n",
    "# Multiply by 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285e65a",
   "metadata": {},
   "source": [
    "# 6. Get monthly AQI estimates for Leavenworth Kansas by combining the nearest four station particulate data.\n",
    "\n",
    "What is AQI?\n",
    "https://ecology.wa.gov/Research-Data/Monitoring-assessment/Air-Quality-Index  \n",
    "AQI stands for Air Quality Index, which is just a number from 0-500(?) that quantifies how hazardous\n",
    "the air currently is. AQI measures carbon monoxide, nitrogren dioxide, ozone health, particle pollution, and sulfur dioxide. \n",
    "\n",
    "In this work, since we are dealing with wildfire data, we only will consider particulates when dealing with AQI. More specifically, we will only include data that measures PM10 and PM2.5 which respectively measure particles less than 10 micrometers and particles less than 2.5 micrometers in size (diameter?).\n",
    "\n",
    "This code was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.1 - September 5, 2023\n",
    "\n",
    "Documentation for the AQS API can be found here: https://aqs.epa.gov/aqsweb/documents/data_api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1dd6a5",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is all of the different constants needed to make use of the AQS API\n",
    "USERNAME = \"zbowyer@uw.edu\"\n",
    "APIKEY = \"tealmallard53\"  #I know this is bad but there's nothing at stake here\n",
    "\n",
    "#Request API Key constants\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "API_ACTION_SIGNUP = '/signup?email=zbowyer@uw.edu'\n",
    "\n",
    "# List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "\n",
    "# Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "\n",
    "# Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "\n",
    "# Assuming roughly 2ms latency on the API and network\n",
    "API_LATENCY_ASSUMED = 0.002 \n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}\n",
    "\n",
    "# Parameter list or 'param' value as defined by the AQS API spec.\n",
    "# It turns out that we want all of these measures for AQI, but we need to have two different param constants to get\n",
    "# all seven of the code types. We can only have a max of 5 sensors/values request per param.\n",
    "\n",
    "# Gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "\n",
    "# Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\"\n",
    "\n",
    "# Information about Leavenworth, Kansas\n",
    "CITY_LOCATIONS = {\n",
    "    'Leavenworth' :    {'city'   : 'Leavenworth',\n",
    "                       'county' : 'Leavenworth',\n",
    "                       'state'  : 'Kansas',\n",
    "                       'fips'   : '20103',\n",
    "                       'latlon' : [39.313015, -94.941147]},\n",
    "}\n",
    "\n",
    "# Used to grab data we need later\n",
    "EXTRACTION_FIELDS = ['sample_duration','observation_count','arithmetic_mean','aqi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc96fd8",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_signup(email_address = None,\n",
    "                   endpoint_url = API_REQUEST_URL, \n",
    "                   endpoint_action = API_ACTION_SIGNUP, \n",
    "                   request_template = AQS_REQUEST_TEMPLATE,\n",
    "                   headers = None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Requests an api key using your email address.\n",
    "    Inputs:\n",
    "        email_address - String\n",
    "        endpoint_url - String\n",
    "        endpoint_action - String\n",
    "        request_template - Dictionary\n",
    "    Outputs:\n",
    "        json_response - Dictionary\n",
    "    Notes:\n",
    "        Modification of function created by David W. McDonald.\n",
    "        This code was developed by David W. McDonald for use in Data 512,\n",
    "        a course in the UW MS Data Science degree program. This code is \n",
    "        provided under the Createive Commons CC-BY license. Revision 1.0 - \n",
    "        September 5, 2023.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Confirm if email address was properly supplied, if so put it in the request\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address        \n",
    "    if not request_template['email']: \n",
    "        raise Exception(\"Must supply an email address to call 'request_signup()'\")\n",
    "    \n",
    "    # Compose the signup url - create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "def request_list_info(email_address = None, key = None,\n",
    "                      endpoint_url = API_REQUEST_URL, \n",
    "                      endpoint_action = API_ACTION_LIST_CLASSES, \n",
    "                      request_template = AQS_REQUEST_TEMPLATE,\n",
    "                      headers = None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Implementation of a \"list request\"\n",
    "    Inputs:\n",
    "        email_address - String\n",
    "        key - String (API key)\n",
    "        endpoint_url - String\n",
    "        endpoint_action - String\n",
    "        request_template - Dictionary\n",
    "        headers - ?\n",
    "    Outputs:\n",
    "        json_response - Dictionary\n",
    "    Notes:\n",
    "        Modification of function created by David W. McDonald.\n",
    "        This code was developed by David W. McDonald for use in Data 512,\n",
    "        a course in the UW MS Data Science degree program. This code is \n",
    "        provided under the Createive Commons CC-BY license. Revision 1.0 - \n",
    "        September 5, 2023.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure we have email and key - at least\n",
    "    # This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    \n",
    "    # For the basic request we need an email address and a key\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_list_info()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_list_info()'\")\n",
    "\n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This implements the daily summary request. \n",
    "        Daily summary provides a daily summary value for each sensor being requested\n",
    "        from the start date to the end date.  Like the two other functions, \n",
    "        this can be called with a mixture of a defined parameter dictionary, or with function\n",
    "        parameters. If function parameters are provided, \n",
    "        those take precedence over any parameters from the request template.\n",
    "    Inputs:\n",
    "        email_address - string\n",
    "        key - string\n",
    "        param - ?\n",
    "        begin_date - string\n",
    "        end_date - string\n",
    "        fips - string\n",
    "        endpoint_url - string\n",
    "        endpoint_action - string\n",
    "        request_template - dictionary\n",
    "        headers - ?\n",
    "    Outputs:\n",
    "        json_response - Dictionary\n",
    "    Notes:\n",
    "        Modification of function created by David W. McDonald.\n",
    "        This code was developed by David W. McDonald for use in Data 512,\n",
    "        a course in the UW MS Data Science degree program. This code is \n",
    "        provided under the Createive Commons CC-BY license. Revision 1.0 - \n",
    "        September 5, 2023.\n",
    "    \"\"\"\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "        \n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "def extract_summary_from_response(r=None, fields=EXTRACTION_FIELDS):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the output of request_daily_summary() and extracts/summarizes based on supplied fields.\n",
    "    Inputs:\n",
    "        r - Dictionary\n",
    "        fields - List of strings\n",
    "    Output:\n",
    "       result - dictionary\n",
    "    Notes:\n",
    "        Modification of function created by David W. McDonald.\n",
    "        This code was developed by David W. McDonald for use in Data 512,\n",
    "        a course in the UW MS Data Science degree program. This code is \n",
    "        provided under the Createive Commons CC-BY license. Revision 1.0 - \n",
    "        September 5, 2023.\n",
    "    \"\"\"\n",
    "    ## the result will be structured around monitoring site, parameter, and then date\n",
    "    result = dict()\n",
    "    data = r[\"Data\"]\n",
    "    for record in data:\n",
    "        # make sure the record is set up\n",
    "        site = record['site_number']\n",
    "        param = record['parameter_code']\n",
    "        #date = record['date_local']    # this version keeps the respnse value YYYY-\n",
    "        date = record['date_local'].replace('-','') # this puts it in YYYYMMDD format\n",
    "\n",
    "        if site not in result:\n",
    "            result[site] = dict()\n",
    "            result[site]['local_site_name'] = record['local_site_name']\n",
    "            result[site]['site_address'] = record['site_address']\n",
    "            result[site]['state'] = record['state']\n",
    "            result[site]['county'] = record['county']\n",
    "            result[site]['city'] = record['city']\n",
    "            result[site]['pollutant_type'] = dict()\n",
    "        if param not in result[site]['pollutant_type']:\n",
    "            result[site]['pollutant_type'][param] = dict()\n",
    "            result[site]['pollutant_type'][param]['parameter_name'] = record['parameter']\n",
    "            result[site]['pollutant_type'][param]['units_of_measure'] = record['units_of_measure']\n",
    "            result[site]['pollutant_type'][param]['method'] = record['method']\n",
    "            result[site]['pollutant_type'][param]['data'] = dict()\n",
    "        if date not in result[site]['pollutant_type'][param]['data']:\n",
    "            result[site]['pollutant_type'][param]['data'][date] = list()\n",
    "        \n",
    "        # now extract the specified fields\n",
    "        extract = dict()\n",
    "        for k in fields:\n",
    "            if str(k) in record:\n",
    "                extract[str(k)] = record[k]\n",
    "            else:\n",
    "                # this makes sure we always have the requested fields, even if\n",
    "                # we have a missing value for a given day/month\n",
    "                extract[str(k)] = None\n",
    "        \n",
    "        # add this extraction to the list for the day\n",
    "        result[site]['pollutant_type'][param]['data'][date].append(extract)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def bounding_latlon(place=None,scale=1.0):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Compute a rough estimates for a bounding box around a given place\n",
    "        The bounding box is scaled in 50 mile increments. \n",
    "        That is the bounding box will have sides that\n",
    "        are rough multiples of 50 miles, with the center of the box around the indicated place.\n",
    "        The scale parameter determines the scale (size) of the bounding box\n",
    "    Inputs:\n",
    "        place - dictionary\n",
    "        scale - float\n",
    "    Output:\n",
    "       result - list of 4 floats\n",
    "    Notes:\n",
    "        Modification of function created by David W. McDonald.\n",
    "        This code was developed by David W. McDonald for use in Data 512,\n",
    "        a course in the UW MS Data Science degree program. This code is \n",
    "        provided under the Createive Commons CC-BY license. Revision 1.0 - \n",
    "        September 5, 2023.\n",
    "    \"\"\"\n",
    "    LAT_25MILES = 25.0 * (1.0/69.0)    # This is about 25 miles of latitude in decimal degrees\n",
    "    LON_25MILES = 25.0 * (1.0/54.6)    # This is about 25 miles of longitude in decimal degrees\n",
    "    minlat = place['latlon'][0] - float(scale) * LAT_25MILES\n",
    "    maxlat = place['latlon'][0] + float(scale) * LAT_25MILES\n",
    "    minlon = place['latlon'][1] - float(scale) * LON_25MILES\n",
    "    maxlon = place['latlon'][1] + float(scale) * LON_25MILES\n",
    "    return [minlat,maxlat,minlon,maxlon]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007761ba",
   "metadata": {},
   "source": [
    "#### Request api signup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Requesting SIGNUP ...\")\n",
    "#response = request_signup(\"zbowyer@uw.edu\")\n",
    "#print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6ff5f",
   "metadata": {},
   "source": [
    "#### Example of monthly summary of stations in Leavenworth County\n",
    "However they dont give particulate data, which is what we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674e805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = CITY_LOCATIONS['Leavenworth']['fips'][:2]\n",
    "request_data['county'] = CITY_LOCATIONS['Leavenworth']['fips'][2:]\n",
    "\n",
    "# request daily summary data for the month of July in 2021\n",
    "request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "particulate_aqi = request_daily_summary(request_template=request_data, begin_date=\"20190701\", end_date=\"20190731\")\n",
    "if particulate_aqi[\"Header\"][0]['status'].startswith(\"No data \"):\n",
    "    print(\"Looks like the response generated no data for particulates. You might take a closer look at your request and the response data.\")\n",
    "\n",
    "extract_particulate = extract_summary_from_response(particulate_aqi)\n",
    "print(\"Summary of particulate extraction ...\")\n",
    "print(json.dumps(extract_particulate,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467d3f0",
   "metadata": {},
   "source": [
    "#### Function to get monthly AQI of Leavenworth estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d939cdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_AQI_estimate_for_month(begin_date, end_date):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the average daily AQI for all stations close to Leavenworth, Kansas for an entire month.\n",
    "    Inputs:\n",
    "        begin-date - string\n",
    "        end-date - string\n",
    "    Outputs:\n",
    "        float\n",
    "    Notes:\n",
    "        begin-date and end-date must be valid dates in the format \"yyyymmdd\"\n",
    "        the dates also can't be greater than a month in difference?\n",
    "    \"\"\"\n",
    "    \n",
    "    #Make request dictionary\n",
    "    request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "    request_data['email'] = USERNAME\n",
    "    request_data['key'] = APIKEY\n",
    "    bbox = bounding_latlon(CITY_LOCATIONS['Leavenworth'],scale=10.0)\n",
    "    request_data['minlat'] = bbox[0]\n",
    "    request_data['maxlat'] = bbox[1]\n",
    "    request_data['minlon'] = bbox[2]\n",
    "    request_data['maxlon'] = bbox[3]\n",
    "    request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "    \n",
    "    # Get daily summary (for a month) of all stations that fall in bounding box area\n",
    "    particulate_aqi = request_daily_summary(request_template=request_data, begin_date=begin_date, end_date=end_date, endpoint_action = API_ACTION_DAILY_SUMMARY_BOX)\n",
    "    \n",
    "    # Filter to only info we care about\n",
    "    extract_particulate = extract_summary_from_response(particulate_aqi)\n",
    "    \n",
    "    # For each station, get the average of (the average PM10 AQI for that month and\n",
    "    # the average PM2 AQI for that month). The final AQI will be the station with \n",
    "    # the highest AQI that month. \n",
    "    \n",
    "    #Loop over each station\n",
    "    largest_aqi_monthly_estimate = 0 \n",
    "    for key in extract_particulate.keys():\n",
    "        station = extract_particulate[key]\n",
    "        station_state = station[\"state\"]\n",
    "        station_county = station[\"county\"]\n",
    "        station_city = station[\"city\"]\n",
    "        #For each day, get 24-HR BLK AVG of 81102 and 88101 and avg their aqi\n",
    "        \n",
    "        reported_avg_monthly_81102 = 0\n",
    "        reported_avg_monthly_88101 = 0 \n",
    "        estimated_avg_pp = 0\n",
    "        \n",
    "        # Get average monthly PM10 - 81102 (PM10 Total 0-10um STP)\n",
    "        if(\"81102\" in station[\"pollutant_type\"].keys()):\n",
    "            counter = 0\n",
    "            for key in station[\"pollutant_type\"][\"81102\"][\"data\"].keys():\n",
    "                day = station[\"pollutant_type\"][\"81102\"][\"data\"][key]\n",
    "                for i in range(len(day)):\n",
    "                    if(day[i][\"sample_duration\"] == \"24-HR BLK AVG\"):\n",
    "                        aqi = day[i]['aqi']\n",
    "                        reported_avg_monthly_81102 += aqi\n",
    "                        counter += 1\n",
    "            if(counter != 0):\n",
    "                reported_avg_monthly_81102 /= counter\n",
    "        \n",
    "        # Get average monthly PM2 - 81102 (PM2 Total 0-2um STP)\n",
    "        if(\"88101\" in station[\"pollutant_type\"].keys()):\n",
    "            counter = 0\n",
    "            for key in station[\"pollutant_type\"][\"88101\"][\"data\"].keys():\n",
    "                day = station[\"pollutant_type\"][\"88101\"][\"data\"][key]\n",
    "                for i in range(len(day)):\n",
    "                    if(day[i][\"sample_duration\"] == \"24-HR BLK AVG\"):\n",
    "                        aqi = day[i]['aqi']\n",
    "                        reported_avg_monthly_88101 += aqi\n",
    "                        counter += 1\n",
    "            if(counter != 0):\n",
    "                reported_avg_monthly_88101 /= counter\n",
    "        \n",
    "        # Average the average monthly PM10 average monthly PM2\n",
    "        estimated_avg_pp = (reported_avg_monthly_81102 + reported_avg_monthly_88101) / 2\n",
    "        \n",
    "        # If this station has the largest monthly average AQI, keep track of it\n",
    "        if(estimated_avg_pp) > largest_aqi_monthly_estimate:\n",
    "            largest_aqi_monthly_estimate = estimated_avg_pp\n",
    "            \n",
    "    #Return largest monthly average AQI from all the stations\n",
    "    return largest_aqi_monthly_estimate\n",
    "\n",
    "# Test out our AQI estimate\n",
    "aqi_est = get_AQI_estimate_for_month(\"20210201\", \"20210128\")\n",
    "print(\"AQI estimate\")\n",
    "print(aqi_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba221b45",
   "metadata": {},
   "source": [
    "#### Create dictionary that holds all monthly AQI estimates for Leavenworth from 1963-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1ecce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aqi_estimates = {}\n",
    "month_strs = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "#For each year...\n",
    "for year in tqdm(range(1963,2023), position=0, leave=True):\n",
    "    #For each month...\n",
    "    for month in month_strs:\n",
    "        #Form a specific date strings that are valid dates...\n",
    "        end_day = \"31\"\n",
    "        if(month == \"02\"):\n",
    "            end_day = \"28\"\n",
    "        if(month == \"04\" or month == \"06\" or month == \"09\" or month == \"11\"):\n",
    "            end_day = \"30\"\n",
    "        begin_date_str = str(year) + month + \"01\"\n",
    "        end_date_str = str(year) + month + end_day\n",
    "        \n",
    "        #Make AQI estimate for supplied dates\n",
    "        aqi_est = get_AQI_estimate_for_month(begin_date_str, end_date_str)\n",
    "        aqi_estimates[begin_date_str] = aqi_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb2ede",
   "metadata": {},
   "source": [
    "# 7. Create linear regression model to predict the annual smoke estimate, and create ARIMA model to forecast the annual smoke estimate to 2049.\n",
    "\n",
    "If predictors exist, do a linear regression.\n",
    "If predictors do not exist, do an ARIMA forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0996c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get data into dataframe and numpy arrays\n",
    "dates = []\n",
    "distances = []\n",
    "acres = []\n",
    "smoke_estimates = []\n",
    "for year in annual_smoke_estimates.keys():\n",
    "    dates.append(str(year) + \"-1-1\")\n",
    "    total_acres = 0\n",
    "    total_distance = 0\n",
    "    if(str(year) in fires_grouped_by_year.keys()):\n",
    "        for fire in fires_grouped_by_year[str(year)]:\n",
    "            if('distance_from_Leavenworth') not in fire.keys():\n",
    "                continue\n",
    "            GIS_Acres = fire['attributes']['GIS_Acres']\n",
    "            distance = fire['distance_from_Leavenworth']\n",
    "            total_acres += GIS_Acres\n",
    "            total_distance += distance\n",
    "    distances.append(total_distance)\n",
    "    acres.append(total_acres)\n",
    "    smoke_estimates.append(annual_smoke_estimates[year]['total_smoke_volume_estimate'])\n",
    "dates = np.array(dates)\n",
    "distances = np.array(distances)\n",
    "acres = np.array(acres)\n",
    "smoke_estimates = np.array(smoke_estimates)\n",
    "combined_data_numpy = (np.dstack([dates, distances, acres, smoke_estimates]))\n",
    "df = pd.DataFrame(data = combined_data_numpy[0], columns = ['date', 'sum_distance', 'sum_acres', 'smoke_est'])\n",
    "\n",
    "#Linear regression model\n",
    "print(\"Linear regression predictions for when we do have data: \")\n",
    "x = df[['sum_distance', 'sum_acres']]\n",
    "y = df[['smoke_est']]\n",
    "model = sm.OLS(y.astype(float), x.astype(float)).fit()\n",
    "predictions = model.predict(x.astype(float)) \n",
    "\n",
    "#Store dictionary of real and projected results to save to json\n",
    "smoke_real_and_projected = {}\n",
    "\n",
    "print(model.summary())\n",
    "year = 1963\n",
    "for i in range(len(predictions)):\n",
    "    print(\"Year:\", year, \"Real:\", smoke_estimates[i], \"Predicted:\", predictions[i])\n",
    "    smoke_real_and_projected[year] = smoke_estimates[i]\n",
    "    year += 1\n",
    "\n",
    "#Forecast using ARIMA\n",
    "print(\"Smoke estimate forecasting: \")\n",
    "model = statsmodels.tsa.arima.model.ARIMA(y.astype(float), order=(3,0,0)).fit()\n",
    "predictions = model.predict(start=60, end=89)\n",
    "print(model.summary())\n",
    "for pred in predictions:\n",
    "    print(\"Year:\", year, \"Predicted:\", pred)\n",
    "    smoke_real_and_projected[year] = pred\n",
    "    year += 1\n",
    "\n",
    "#Save dictionary as json\n",
    "print(smoke_real_and_projected)\n",
    "with open(\"../Data/SmokeEstimates.json\", \"w\") as outfile: \n",
    "    json.dump(smoke_real_and_projected, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa4116",
   "metadata": {},
   "source": [
    "# 8. Create histogram visualization showing the number of fires occuring every 50 miles from Leavenworth up to 1250 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6599d84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def histogram_number_fires(max_dist):\n",
    "    '''\n",
    "    Description:\n",
    "        Make a matplotlib.pyplot histogram of number of fires \n",
    "        where we only consider bins of size 50\n",
    "        and fires that are closer than max_dist\n",
    "    Inputs:\n",
    "        max_dist - Float\n",
    "    '''\n",
    "\n",
    "    #Get a list of distances\n",
    "    distances = []\n",
    "    for fire in wildfire_data:\n",
    "        try:\n",
    "            distance = fire[\"distance_from_Leavenworth\"]\n",
    "        except:\n",
    "            continue\n",
    "        if(distance < max_dist):\n",
    "            distances.append(distance)\n",
    "    \n",
    "    #Set the number of bins to make them equally spaced out every 50 miles\n",
    "    bins = np.linspace(0, max_dist, ((max_dist) // 50)+1)\n",
    "    \n",
    "    #Create plot\n",
    "    plt.hist(distances, bins, edgecolor='black')\n",
    "    plt.xlabel(\"Miles\")\n",
    "    plt.ylabel(\"Number of fires\")\n",
    "    plt.title(\"Number of wildfires every 50 miles from Leavenworth, Kansas\")\n",
    "    \n",
    "#Create visualization\n",
    "histogram_number_fires(1250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8ebbf",
   "metadata": {},
   "source": [
    "# 9. Create time series showing annual acres burned by wildfires within 1250 miles of Leavenworth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc579f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def time_series_totalacres(max_distance):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Creates a time series graph showing annual acres burned by wildwires\n",
    "        within a specific distance of Leavenworth\n",
    "    Inputs:\n",
    "        max_distance - float\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    acres_burned = []\n",
    "    \n",
    "    #Get years sorted\n",
    "    years = []\n",
    "    for year in fires_grouped_by_year:\n",
    "        years.append(year)\n",
    "    years.sort()\n",
    "    \n",
    "    #For each year...\n",
    "    for year in years:\n",
    "        #Get all fires\n",
    "        fires = fires_grouped_by_year[year]\n",
    "        \n",
    "        #Store full date string that should be in format dd/mm/yyyy\n",
    "        dates.append(\"01/01/\"+ str(year))\n",
    "        \n",
    "        #For all fires, sum the number of burned acres if the fire is in range\n",
    "        total_acres_burned = 0\n",
    "        for fire in fires:\n",
    "            if(\"distance_from_Leavenworth\" in fire.keys()):\n",
    "                distance = fire[\"distance_from_Leavenworth\"]\n",
    "                if(distance < max_distance):\n",
    "                    total_acres_burned += fire[\"attributes\"][\"GIS_Acres\"]\n",
    "        #Store number of acres burned that year\n",
    "        acres_burned.append(total_acres_burned)\n",
    "    \n",
    "    #Convert datestrings to datetime so the time series plot works\n",
    "    dates = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in dates]\n",
    "    \n",
    "    #Plot\n",
    "    plt.plot(dates, acres_burned)\n",
    "    plt.ticklabel_format(style='plain', useOffset=False, axis='y')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Acres\")\n",
    "    plt.title(\"Total acres burned per year from fires occuring no more than\\n \" + str(max_distance) + \" miles from Leavenworth, Kansas\")\n",
    "\n",
    "# Create visualization\n",
    "time_series_totalacres(1250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda1e4a",
   "metadata": {},
   "source": [
    "# 10. Create time series showing smoke estimate, create time series showing AQI estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862530ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get sorted annual dates in datetime format (Just years)\n",
    "smoke_estimates = []\n",
    "dates = []\n",
    "for year in annual_smoke_estimates.keys():\n",
    "    dates.append(\"01/01/\"+ str(year))\n",
    "    smoke_estimates.append(annual_smoke_estimates[year]['total_smoke_volume_estimate'])\n",
    "dates = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in dates]\n",
    "\n",
    "#Get sorted monthly dates in datetime format (years and months)\n",
    "dates2 = []\n",
    "aqi_estimates_list = []\n",
    "for date in aqi_estimates.keys():\n",
    "    year = date[0:4]\n",
    "    month = date[4:6]\n",
    "    day = \"01\"\n",
    "    date_str = month + \"/\" + day + \"/\" + year\n",
    "    dates2.append(date_str)\n",
    "    aqi_estimates_list.append(aqi_estimates[date])\n",
    "dates2 = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in dates2]\n",
    "\n",
    "#Create 2x1 grid of plots\n",
    "fig, ax = plt.subplots(2,1, figsize = (7,10))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "#Plot time series of aqi estimates\n",
    "ax[0].plot(dates2, aqi_estimates_list)\n",
    "ax[0].set_xlabel(\"Year\")\n",
    "ax[0].set_ylabel(\"Estimate AQI\")\n",
    "ax[0].set_title(\"Time series of AQI estimate Leavenworth, Kansas\")\n",
    "\n",
    "#Plot time series of Smoke estimates\n",
    "ax[1].plot(dates, smoke_estimates)\n",
    "ax[1].set_xlabel(\"Year\")\n",
    "ax[1].set_ylabel(\"Estimated smoke impact\")\n",
    "ax[1].set_title(\"Time series of smoke impact estimate Leavenworth, Kansas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df557968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data512Project",
   "language": "python",
   "name": "data512project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
